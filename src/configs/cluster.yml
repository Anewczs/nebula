version: 1.0

# server configs
server:
  # as node will treat server to run as a node
  # if false, server will not load any data in itself.
  # NOTE that, there are maybe missing functions to fully compatible for server as node.
  # such as task execution may not be implemented.
  anode: false

# will be provided by enviroment
nodes:
  - node:
      host: 10.1.179.237
      port: 9199

tables:
  nebula.test:
    # max 10G RAM assigment
    max-mb: 10000
    # max 10 days assignment
    max-hr: 240
    schema: "ROW<id:int, event:string, items:list<string>, flag:bool, value:tinyint>"
    data: custom
    loader: NebulaTest
    source: ""
    backup: s3://nebula/n100/
    format: none
    time:
      type: static
      # get it from linux by "date +%s"
      value: 1565994194

  # pin.comments:
  #   max-mb: 40000
  #   max-hr: 0
  #   schema: "ROW<pin_signature:string, user_id:long, comments:string, created_at:string>"
  #   data: s3
  #   loader: Swap
  #   source: s3://<bucket>/nebula/pin_comments/
  #   backup: s3://nebula/n101/
  #   format: csv
  #   columns:
  #     user_id:
  #       bloom_filter: true
  #     pin_signature:
  #       bloom_filter: true
  #   time:
  #     type: column
  #     column: created_at
  #     pattern: "%Y-%m-%d %H:%M:%S"

  # pin.signatures:
  #   max-mb: 40000
  #   max-hr: 0
  #   schema: "ROW<pin_signature:string, pin_id:long>"
  #   data: s3
  #   loader: Swap
  #   source: s3://<bucket>/nebula/pin_signatures/
  #   backup: s3://nebula/n102/
  #   format: csv
  #   columns:
  #     pin_signature:
  #       bloom_filter: true
  #     pin_id:
  #       bloom_filter: true
  #   time:
  #     type: current

  # pin.pins:
  #   max-mb: 200000
  #   max-hr: 48
  #   schema: "ROW<id:long, user_id:long, link_domain:string, title:string, details:string, image_signature:string>"
  #   data: s3
  #   loader: Roll
  #   source: s3://<bucket>/nebula/pin_pins/cd=%7Bdate%7D
  #   backup: s3://nebula/n103/
  #   format: parquet
  #   columns:
  #     id:
  #       bloom_filter: true
  #     user_id:
  #       bloom_filter: true
  #     link_domain:
  #       dict: true
  #   time:
  #     type: macro
  #     pattern: date

  # s3.prefix:
  #   max-mb: 200000
  #   max-hr: 0
  #   schema: "ROW<prefix:string, operation_type:string, requests:long, size:long>"
  #   data: s3
  #   loader: Swap
  #   source: s3://<bucket>/nebula/s3_pfx/
  #   backup: s3://nebula/n104/
  #   format: parquet
  #   columns:
  #     prefix:
  #       dict: true
  #     operation_type:
  #       dict: true
  #   time:
  #     type: static
  #     # date +%s --date='2019-09-10'
  #     value: 1568073600

  ## all kafka table name will start with "k." followed by topic name
  ## to indicating its a kafka data source
  k.<topic>:
    max-mb: 200000
    max-hr: 12
    # all kafka schema will have column name postfixed by field ID
    # TODO(cao): in the future, we can have separate thrift schema
    schema: "ROW<userId:long, magicType:short, statusCode:byte, objectCount:int>"
    data: kafka
    topic: <topic>
    loader: Roll
    source: <brokers>
    backup: s3://nebula/n105/
    format: thrift
    serde:
      protocol: binary
      cmap:
        # TODO(cao): this temporary hack to work around nested thrift definition
        # we're using 1K to separate two levels asssuming no thrift definition has more than 1K fields
        # in reverse order, such as 1003 => (field 3 -> field 1)
        _time_: 1
        userId: 3001
        magicType: 3003
        statusCode: 4002
        objectCount: 4001
    columns:
      userId:
        bloom_filter: true
      statusCode:
        default_value: 0
      objectCount:
        default_value: 0
    time:
      type: current
